{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "853d7bb5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 27\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StratifiedKFold\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m\n\u001b[1;32m     29\u001b[0m pd\u001b[38;5;241m.\u001b[39mset_option(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdisplay.max_colwidth\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m200\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from numpy import array, argmax, random, take\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Embedding, RepeatVector\n",
    "#from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import load_model\n",
    "from keras import optimizers\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "import plotly.express as px\n",
    "#px.offline.init_notebook_mode(connected=True)\n",
    "from IPython.core.display import HTML\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import torch\n",
    "import random\n",
    "pd.set_option('display.max_colwidth', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2757dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install googletrans\n",
    "from googletrans import Translator\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f03c48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"C:\\\\Users\\\\Abd el rahman\\\\Desktop\\\\NLP_DataSet\\\\fra.txt\"\n",
    "with open(data_path, 'r' , encoding='utf-8') as f:\n",
    "    lines = f.read()\n",
    "lines \n",
    "#data_train_path = 'C:\\\\Users\\\\Abd el rahman\\\\Desktop\\\\New folder\\\\MY_data\\\\train'\n",
    "#data_test_path='C:\\\\Users\\\\Abd el rahman\\\\Desktop\\\\New folder\\\\MY_data\\\\test'\n",
    "#\"C:\\Users\\Abd el rahman\\Desktop\\NLP_DataSet\\fra.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddeb4788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to read raw text file\n",
    "def read_text(filename):\n",
    "        # open the file\n",
    "        file = open(filename, mode='rt', encoding='utf-8')\n",
    "        \n",
    "        # read all text\n",
    "        text = file.read()\n",
    "        file.close()\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c797ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory\n",
    "#DATA_DIR = DATA_DIR = \"C:\\\\Users\\\\Abd el rahman\\\\Desktop\\\\NLP_DataSet\"\n",
    "\n",
    "# read train and test csv files\n",
    "#train_df = pd.read_csv(os.path.join(DATA_DIR, \"train_dataset.csv\"))\n",
    "#test_df = pd.read_csv(os.path.join(DATA_DIR, \"test_dataset.csv\"))\n",
    "#print(f\"train data {train_df.shape} test data {test_df.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0990dd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_data, test_data = train_test_split(train_df, test_size=0.3, random_state=42)\n",
    "#print(f\"train data {train_df.shape} test data {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1dbddd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset_size = {\"train set\":train_df.shape[0], \"test set\":test_df.shape[0]}\n",
    "#fig = px.bar(y = list(dataset_size.keys()), x = list(dataset_size.values()), \n",
    "#             title=\"Distribution of train and test\", text= list(dataset_size.values()))\n",
    "#fig.update_layout(\n",
    "#    xaxis_title=\"No of samples\",\n",
    "#    yaxis_title=\"Dataset\")\n",
    "#fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d15ae24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_lines(text):\n",
    "    sents = text.strip().split('\\n')\n",
    "    sents = [i.split('\\t') for i in sents]\n",
    "    return sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e81202",
   "metadata": {},
   "outputs": [],
   "source": [
    "fra_eng = to_lines(lines)\n",
    "fra_eng[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c425428",
   "metadata": {},
   "outputs": [],
   "source": [
    "fra_eng = array(fra_eng)\n",
    "fra_eng[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9767c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fra_eng.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d633362",
   "metadata": {},
   "outputs": [],
   "source": [
    "fra_eng = fra_eng[:90000,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91240d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fra_eng = fra_eng[:,[0,1]]\n",
    "fra_eng[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e847865",
   "metadata": {},
   "outputs": [],
   "source": [
    "fra_eng[:,0] = [s.translate(str.maketrans('', '', string.punctuation))\n",
    "               for s in fra_eng[:,0]]\n",
    "fra_eng[:,1] = [s.translate(str.maketrans('', '', string.punctuation))\n",
    "               for s in fra_eng[:,1]]\n",
    "fra_eng[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3bafa3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(fra_eng)):\n",
    "    fra_eng[i:0] = fra_eng[i,0].lower()\n",
    "    fra_eng[i:1] = fra_eng[i,1].lower()\n",
    "\n",
    "fra_eng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27dbd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer\n",
    "eng_tokenizer = tokenization(fra_eng[:,0])\n",
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "\n",
    "eng_length = 8\n",
    "print('English Vocabulary Size: %d' % eng_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24eaf824",
   "metadata": {},
   "outputs": [],
   "source": [
    "fra_tokenizer = tokenization(fra_eng[:,1])\n",
    "fra_vocab_size = len(fra_tokenizer.word_index) + 1\n",
    "\n",
    "fra_length = 8\n",
    "print('French Vocabulary Size: %d' % fra_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238649a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sequences(tokenizer, length, lines):\n",
    "    seq = tokenizer.texts_to_sequences(lines)\n",
    "    seq = pad_sequences(seq, maxlen=length , padding='post')\n",
    "    return seq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b2cd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#split data into train and test set\n",
    "train, test = train_test_split(fra_eng ,test_size=0.2 ,random_state = 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665b4146",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare training data\n",
    "trainX = encode_sequences(fra_tokenizer , fra_length , train[:,1])\n",
    "trainY = encode_sequences(eng_tokenizer , eng_length , train[:,0])\n",
    "\n",
    "#prepare testing data\n",
    "testX = encode_sequences(fra_tokenizer , fra_length , test[:,1])\n",
    "testY = encode_sequences(eng_tokenizer , eng_length , test[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e2330a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#build NMT model\n",
    "def define_model(in_vocab, out_vocab, in_timesteps, out_timesteps, units):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(in_vocab, units, input_length=in_timesteps, mask_zero=True))\n",
    "    model.add(LSTM(units))\n",
    "    model.add(RepeatVector(out_timesteps))\n",
    "    model.add(LSTM(units, return_sequences=True))\n",
    "    model.add(Dense(out_vocab, activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32b670d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "model = define_model(fra_vocab_size, eng_vocab_size, fra_length, eng_length, 512)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3daa68a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "\n",
    "# Define the learning rate\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Define the optimizer with the correct learning rate\n",
    "rms = optimizers.RMSprop(learning_rate=learning_rate)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=rms, loss='sparse_categorical_crossentropy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3e7a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'model.h1.24_jan_19.keras'\n",
    "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "# train model\n",
    "#history = model.fit(trainX, trainY.reshape(trainY.shape[0], trainY.shape[1], 1),\n",
    "#                    epochs=30, batch_size=32, validation_split = 0.2,callbacks=[checkpoint], \n",
    "#                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00429430",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(trainX, trainY.reshape(trainY.shape[0], trainY.shape[1], 1),\n",
    "                   epochs=10, batch_size=512, validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ff9216",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict_classes(testX.reshape((testX.shape[0], testY.shape[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5218f327",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word(n, tokenizer):\n",
    "    for word_index in tokenizer.word_index.items():\n",
    "        if index == n:\n",
    "            return word\n",
    "    return None    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845ab458",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_text = []\n",
    "\n",
    "for i in preds:\n",
    "    temp = []\n",
    "    for j in range(len(i)):\n",
    "        t = get_word(i[j], eng_tokenizer)\n",
    "        if j > 0:\n",
    "            if (t == get_word(i[j-1], eng_tokenizer)) or (t == None):\n",
    "                temp.append('')\n",
    "            else:\n",
    "                temp.append(t)\n",
    "        else:\n",
    "                if(t == None):\n",
    "                    temp.append('')\n",
    "                else:\n",
    "                    temp.append(t)\n",
    "    preds_text.append(' '.join(temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92ff8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pd.DataFrame({'actual' : test[:,0], 'predicted' : preds_text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e053b5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df.sample(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555b9747",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9209df30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
